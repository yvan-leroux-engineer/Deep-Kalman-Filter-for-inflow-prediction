{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Travail de mise en forme des données\r\n",
    "Données téléchargées sur : http://fisher.stats.uwo.ca/faculty/aim/epubs/mhsets/thompsto/\r\n",
    "\r\n",
    "Nous avons les données qui sont disposées par quart de mois, soit 48 données par année, sur 30 ans (1953 à 1982), donc un total de 1440 observations\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from torch import optim\r\n",
    "from attrdict import AttrDict\r\n",
    "import tqdm\r\n",
    "\r\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
    "print(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Définition des constantes\r\n",
    "N_UNITES_PAR_MOIS = 4\r\n",
    "N_UNITES_PAR_AN = 12 * N_UNITES_PAR_MOIS\r\n",
    "N_ANNEE_DATASET = 30\r\n",
    "N_UNITES_DANS_DATASET = N_ANNEE_DATASET * N_UNITES_PAR_AN\r\n",
    "\r\n",
    "#On crée la séquence de temps discrétiser sur 30 ans et chaque année elle même discrétiser sur 48 quarts de mois\r\n",
    "Big_Data = np.empty(shape=(N_UNITES_DANS_DATASET,5))\r\n",
    "for i in range(N_ANNEE_DATASET):\r\n",
    "    for j in range(N_UNITES_PAR_AN):\r\n",
    "        Big_Data[j+i*N_UNITES_PAR_AN][0]=1953+i\r\n",
    "        Big_Data[j+i*N_UNITES_PAR_AN][1]=j+1\r\n",
    "\r\n",
    "#On importe les valeurs des fichiers, et on traite les donnéees de telle sorte à n'avoir qu'une colonne de 1440 observations\r\n",
    "Files = [\"lacstjin.txt\",\"lacstjra.txt\",\"lacstjsn.txt\"]\r\n",
    "for count,file in enumerate(Files) :\r\n",
    "    data = np.reshape(np.loadtxt(\"./Data/\"+file), newshape=(1,1440))\r\n",
    "    Big_Data[:,count+2] = (data.copy() - data.min(axis=1))/(data.max(axis=1)-data.min(axis=1)) \r\n",
    "    \r\n",
    "Big_Data = pd.DataFrame(Big_Data, columns=['Years','Quarters','Inflow','Rainfall','Melt of Ice'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Premiers Graphiques sur les données brutes\n",
    "\n",
    "Ici on chercher à sortir des graphiques sur les données en mettant le temps en abcisse afin d'apporter des premières réflexions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tracé de l'ensemble du jeu de donnée"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Création de la liste des années à accoller aux quarts de mois\r\n",
    "Time_Stamp = ['Y:'+str(1953+i) for i in range(N_ANNEE_DATASET)]\r\n",
    "Time_Stamp_x = [(i+.5)*N_UNITES_PAR_AN for i in range(N_ANNEE_DATASET)]\r\n",
    "timing = [i for i in range(N_UNITES_DANS_DATASET)]\r\n",
    "\r\n",
    "titles  = ['Inflow','Rainfall','Melt of Ice']\r\n",
    "colors = ['blue','green','orange']\r\n",
    "\r\n",
    "#Plot des données\r\n",
    "for t,c in zip(titles,colors):\r\n",
    "    fig, ax = plt.subplots(figsize=(40,5))\r\n",
    "    ax.plot(timing, Big_Data[t], color = c)\r\n",
    "    ax.set_xlim(0,N_UNITES_DANS_DATASET)\r\n",
    "    ax.set_ylim(0,1)\r\n",
    "    for i in range(N_ANNEE_DATASET):\r\n",
    "        if i%2==0:\r\n",
    "            ax.axvspan(i*N_UNITES_PAR_AN, (i+1)*N_UNITES_PAR_AN, ec='black',fill=False)\r\n",
    "    \r\n",
    "    plt.xticks(Time_Stamp_x,Time_Stamp)\r\n",
    "    plt.margins(0.2)\r\n",
    "    plt.subplots_adjust(bottom=0.15)\r\n",
    "    plt.title(f'Quarters Monthly {t} over the 30 years')\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparaison par année\n",
    "\n",
    "On remarque dans la mise en graphique des données brutes une certaine périodicité des valeurs, dans les prochaines étapes nous allons superposer les données par année afin de vérifier ce propos :"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Liste des abscisses fixe sur l'année\r\n",
    "X = Big_Data['Quarters'].iloc[0:N_UNITES_PAR_AN]\r\n",
    "\r\n",
    "for title in titles:\r\n",
    "    #Plot de Inflow sur fenêtre d'un an\r\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\r\n",
    "    for i in range(N_ANNEE_DATASET):\r\n",
    "        Y = Big_Data[title].iloc[N_UNITES_PAR_AN*i:N_UNITES_PAR_AN*(i+1)]\r\n",
    "        labels = Big_Data['Years'].iloc[N_UNITES_PAR_AN*i]\r\n",
    "        ax.plot(X , Y, label=labels)\r\n",
    "    #Moyenne transversale\r\n",
    "    title_mean_serie = [np.mean([Big_Data[title].iloc[i*N_UNITES_PAR_AN+j] for i in range(N_ANNEE_DATASET)]) for j in range(N_UNITES_PAR_AN)]\r\n",
    "    ax.plot(X, title_mean_serie, color='black', linestyle='dashed', linewidth=3,label=\"Mean\")\r\n",
    "    plt.legend()\r\n",
    "    plt.title(f'Quarters monthly {title} per year')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Superposition des données \r\n",
    "\r\n",
    "Maintenant que l'on a montré que les données suivent la réalité scientifique, avec des tendances saisonnière remarquable sur la plupart des années, nous allons superposer les données normalisées de quelques années afin d'observer les liens de causes à effets sur le débit du fleuve"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Choix des années à plot\r\n",
    "I = [0,9,19,29]\r\n",
    "\r\n",
    "#Plot des 4 années selectionnées\r\n",
    "fig, ax = plt.subplots(figsize=(20,5))\r\n",
    "\r\n",
    "# Liste des abscisses fixe sur l'année\r\n",
    "X = Big_Data['Quarters'].iloc[0:N_UNITES_PAR_AN]\r\n",
    "\r\n",
    "for l,i in enumerate(I):\r\n",
    "    plt.subplot(1,len(I),l+1)\r\n",
    "    for title,color in zip(titles,colors):\r\n",
    "        plt.plot(X, Big_Data[title].iloc[N_UNITES_PAR_AN*i:N_UNITES_PAR_AN*(i+1)], label=title,color=color, linewidth=1)\r\n",
    "    plt.legend()\r\n",
    "    plt.title('Year %s' % (int(Big_Data['Years'].iloc[N_UNITES_PAR_AN*i])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Préparation des données\r\n",
    "Découpage des données d'observations et d'actions pour les phases:\r\n",
    "- d'entraînement sur les 26 premières années\r\n",
    "- de validation sur la 27 et 28 ème année\r\n",
    "- de test sur les 2 années restantes\r\n",
    "\r\n",
    "Sur chacun de ces découpages, nous allons faire glisser une fenêtre correspondant à certain nombre de mois afin d'augmenter nos données."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def dataset_train_valid_test (X,U,config):\r\n",
    "    \r\n",
    "    # Données de configuration extraites afin qu'elles soient explicites\r\n",
    "    N_ANNEE_ENTRAINEMENT = config.n_annee_entrainement*N_UNITES_PAR_AN\r\n",
    "    N_ANNEE_VALIDATION = N_ANNEE_ENTRAINEMENT + config.n_annee_validation*N_UNITES_PAR_AN\r\n",
    "    batch_size = config.n_mois*N_UNITES_PAR_MOIS\r\n",
    "    \r\n",
    "    # Découpage des données pour le jeu d'entrainement\r\n",
    "    X_train = np.lib.stride_tricks.sliding_window_view(X[:N_ANNEE_ENTRAINEMENT],batch_size)\r\n",
    "    U_train = np.lib.stride_tricks.sliding_window_view(U[:N_ANNEE_ENTRAINEMENT],(batch_size,U.shape[1]))\r\n",
    "    U_train = U_train.reshape(N_ANNEE_ENTRAINEMENT-batch_size+1,batch_size,U.shape[1])[:-1]\r\n",
    "    \r\n",
    "    # Découpage des données pour le jeu de validation\r\n",
    "    X_validation = np.lib.stride_tricks.sliding_window_view(X[N_ANNEE_ENTRAINEMENT-batch_size:N_ANNEE_VALIDATION],batch_size)\r\n",
    "    U_validation = np.lib.stride_tricks.sliding_window_view(U[N_ANNEE_ENTRAINEMENT-batch_size:N_ANNEE_VALIDATION],(batch_size,U.shape[1]))\r\n",
    "    U_validation = U_validation.reshape(N_ANNEE_VALIDATION-N_ANNEE_ENTRAINEMENT+1,batch_size,U.shape[1])[:-1]\r\n",
    "    \r\n",
    "    # Découpage des données pour le jeu de test\r\n",
    "    X_test = np.lib.stride_tricks.sliding_window_view(X[N_ANNEE_VALIDATION-batch_size:],batch_size)\r\n",
    "    U_test = np.lib.stride_tricks.sliding_window_view(U[N_ANNEE_VALIDATION-batch_size:],(batch_size,U.shape[1]))\r\n",
    "    U_test = U_test.reshape(N_UNITES_DANS_DATASET-N_ANNEE_VALIDATION+1,batch_size,U.shape[1])[:-1]\r\n",
    "    \r\n",
    "    return X_train, U_train, X_validation, U_validation, X_test, U_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = Big_Data['Inflow'].to_numpy()\r\n",
    "U = Big_Data[['Inflow','Rainfall','Melt of Ice']].to_numpy()\r\n",
    "\r\n",
    "config_data = AttrDict({\r\n",
    "    \"n_mois\": 3,\r\n",
    "    \"n_annee_entrainement\": 26,\r\n",
    "    \"n_annee_validation\": 2\r\n",
    "    })\r\n",
    "\r\n",
    "X_train, U_train, X_validation, U_validation, X_test, U_test = dataset_train_valid_test (X,U,config_data)\r\n",
    "\r\n",
    "X_train_torch = torch.Tensor(X_train).unsqueeze(-1).to(device)\r\n",
    "U_train_torch = torch.Tensor(U_train).to(device)\r\n",
    "X_valid_torch = torch.Tensor(X_validation).unsqueeze(-1).to(device)\r\n",
    "U_valid_torch = torch.Tensor(U_validation).to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Kalman Filter\r\n",
    "\r\n",
    "Mise en place d'un réseau d'inférence du filtre Kalman, avec une paramétrisation par MLP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Decodeur (nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    Retourne des observations x_t selon une loi de distribution (à définir soi-même, ici gaussien) avec des paramètres qui sont fonctions de l'espace latent z_t\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self,z_dim,dim_cc,decodeur_dim_sortie): \r\n",
    "        \"\"\"\r\n",
    "        z_dim correspond à la dimension de l'espace latent z_t\r\n",
    "        dim_cc correspond à la dimension des couches cachées complétements connectées\r\n",
    "        decodeur_dim_sortie correspond aux nombres de sorties souhaitées\r\n",
    "        \"\"\"\r\n",
    "        super(Decodeur, self).__init__()\r\n",
    "        self.z_to_cc = nn.Linear(z_dim, dim_cc)\r\n",
    "        self.cc_to_cc = nn.Linear(dim_cc, dim_cc)\r\n",
    "        self.cc_to_mu = nn.Linear(dim_cc, decodeur_dim_sortie)\r\n",
    "        \r\n",
    "        self.relu = nn.ReLU()\r\n",
    "    \r\n",
    "    def forward(self, z_t):\r\n",
    "        \"\"\"\r\n",
    "        On retourne uniquement le vecteur des moyennes paramètres des gaussiennes\r\n",
    "        \"\"\"\r\n",
    "        cc1 = self.relu(self.z_to_cc(z_t))\r\n",
    "        cc2 = self.relu(self.cc_to_cc(cc1))\r\n",
    "        mu = self.cc_to_mu(cc2)\r\n",
    "\r\n",
    "        return mu\r\n",
    "\r\n",
    "class Transition(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    Distribution de l'espace latent z_{t-1} à z_t\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, z_dim, dim_cc):\r\n",
    "        \"\"\"\r\n",
    "        z_dim correspond à la dimension de l'espace latent z_t\r\n",
    "        dim_cc correspond à la dimension des couches cachées complétements connectées\r\n",
    "        \"\"\"\r\n",
    "        super(Transition, self).__init__()\r\n",
    "        self.z_to_cc = nn.Linear(z_dim, dim_cc)\r\n",
    "        self.cc_to_cc = nn.Linear(dim_cc, dim_cc)\r\n",
    "        self.cc_to_mu = nn.Linear(dim_cc, z_dim)\r\n",
    "        self.cc_to_sig = nn.Linear(dim_cc, z_dim)\r\n",
    "\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "        self.softplus = nn.Softplus()\r\n",
    "\r\n",
    "    def forward(self, z_t_1):\r\n",
    "        \"\"\"\r\n",
    "        On retourne la structure de l'espace latent\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        cc1 = self.relu(self.z_to_cc(z_t_1))\r\n",
    "        cc2 = self.relu(self.cc_to_cc(cc1))\r\n",
    "\r\n",
    "        mu = self.cc_to_mu(cc2)\r\n",
    "        sigma = self.softplus(self.cc_to_sig(cc2))\r\n",
    "\r\n",
    "        return mu, sigma\r\n",
    "    \r\n",
    "class Posterieur(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    Ajustement des paramètres de distribution sortie de Transition avec l'intégration des variables d'action, c'est ici que l'on met à profit le principe des filtres de Kalman\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, z_dim, cc_dim, actions_dim):\r\n",
    "        \"\"\"\r\n",
    "        z_dim correspond à la dimension de l'espace latent z_t\r\n",
    "        dim_cc correspond à la dimension des couches cachées complétements connectées\r\n",
    "        actions_dim correspond à la dimension des actions prises en comptes dans le postérieur\r\n",
    "        \"\"\"\r\n",
    "        super(Posterieur, self).__init__()\r\n",
    "        self.z_plus_action_to_cc = nn.Linear(2*z_dim + actions_dim, cc_dim)\r\n",
    "        self.cc_to_cc = nn.Linear(cc_dim, cc_dim)\r\n",
    "        self.cc_to_mu = nn.Linear(cc_dim, z_dim)\r\n",
    "        self.cc_to_sig = nn.Linear(cc_dim, z_dim)\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "        self.softplus = nn.Softplus()\r\n",
    "\r\n",
    "    def forward(self, z_mu, z_sig, action_t):\r\n",
    "        \"\"\"\r\n",
    "        On retourne les nouveaux paramètres de la distribution réajustés\r\n",
    "        \"\"\"\r\n",
    "        cc1 = self.relu(self.z_plus_action_to_cc(torch.cat((z_mu, z_sig, action_t), dim=-1)))\r\n",
    "        cc2 = self.relu(self.cc_to_cc(cc1))\r\n",
    "\r\n",
    "        mu = self.cc_to_mu(cc2)\r\n",
    "        sig = self.softplus(self.cc_to_sig(cc2))\r\n",
    "\r\n",
    "        return mu, sig"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DeepKalmanFilter(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    Mise en place du Deep Kalman Filter\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, config):\r\n",
    "        \"\"\"\r\n",
    "        Variables nécessaires à l'initialisation des modules\r\n",
    "        z_dim                   : dimension de l'espace latent z_t\r\n",
    "        decodeur_dim_cachee     : dimension des couches cachées du décodeur entre l'espace latent et la sortie\r\n",
    "        decodeur_dim_sortie    : dimension de la sortie du décodeur\r\n",
    "        transition_dim_cachee   : dimension des couches cachées du réseau de transition entre l'espace latent z_{t-1} et le postérieur\r\n",
    "        actions_dim             : dimension des actions prises en compte dans le réseau de postérieur\r\n",
    "        posterieur_dim_cachee   : dimension des couches cachées du réseau de postérieur entre la (transition + actions) et l'espace latent z_t\r\n",
    "        \r\n",
    "        \"\"\"\r\n",
    "        super(DeepKalmanFilter, self).__init__()\r\n",
    "\r\n",
    "        self.decodeur = Decodeur(\r\n",
    "            config.z_dim, \r\n",
    "            config.decodeur_dim_cachee, \r\n",
    "            config.decodeur_dim_sortie\r\n",
    "            )\r\n",
    "        \r\n",
    "        self.transition = Transition(\r\n",
    "            config.z_dim, \r\n",
    "            config.transition_dim_cachee\r\n",
    "            )\r\n",
    "\r\n",
    "        self.posterieur = Posterieur(\r\n",
    "            config.z_dim,\r\n",
    "            config.posterieur_dim_cachee,\r\n",
    "            config.actions_dim\r\n",
    "        )\r\n",
    "        # Initialisation du premier espace latent\r\n",
    "        self.z_q_0 = nn.Parameter(torch.zeros(config.z_dim))\r\n",
    "        # Initialisation de l'écart type qui servira à la reconstruction des x_t via le décodeur\r\n",
    "        self.decodeur_sigma = nn.Parameter(config.decodeur_constante*torch.ones(config.decodeur_dim_sortie))\r\n",
    "        #self.decodeur_sigma = nn.Parameter(torch.ones(config.decodeur_dim_sortie))\r\n",
    "        # Dictionnaire contenant toutes les informations nécessaires au fonctionnement de la classe\r\n",
    "        self.config = config\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def reparametrization(mu, sig):\r\n",
    "        \"\"\"\r\n",
    "        Astuce de reparamétrisation après la sortie du postérieur pour permettre la rétropropagation du réseau dans son ensemble\r\n",
    "        \"\"\"\r\n",
    "        return mu + torch.randn_like(sig) * sig\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def kl_div(mu0, sig0, mu1, sig1):\r\n",
    "        \"\"\"\r\n",
    "        Calcul de la divergence entre deux distributions normales paramétrisées par mu0, sig0, mu1, sig1\r\n",
    "        \"\"\"\r\n",
    "        return -0.5 * torch.sum(1 - 2 * sig1.log() + 2 * sig0.log()\r\n",
    "                                - (mu1 - mu0).pow(2) / sig1.pow(2) - (sig0/sig1).pow(2))\r\n",
    "\r\n",
    "    def loss(self, obs, action):\r\n",
    "        \"\"\"\"\r\n",
    "        Définition du calcul de la Loss pour le réseau Deep Kalman Filter\r\n",
    "        obs         : tableau des observations x_t\r\n",
    "        time_step   : 2 ème dimension de obs, longueur de la fenêtre de temps, représente le nombre d'itération sur lesquelles sera entrainé\r\n",
    "        batch_size  : 1ère dimension de obs, échantillon des données globales\r\n",
    "        action      : tableau des actions intégrer dans le postérieur\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        time_step = obs.size(1)\r\n",
    "        batch_size = obs.size(0)\r\n",
    "\r\n",
    "        kl = torch.Tensor([0]).to(self.config.device)\r\n",
    "        reconstruction = torch.Tensor([0]).to(self.config.device)\r\n",
    "        \r\n",
    "        # exponentiel du sigma précédemment défini\r\n",
    "        decodeur_sig = self.decodeur_sigma.exp()\r\n",
    "        # Ajustement de la dimension de l'espace latent en fonction des paramètres indiqués afin d'itérer sur les colonnes du batch\r\n",
    "        z_q_t_1 = self.z_q_0.expand((batch_size-1, self.config.z_dim))\r\n",
    "                \r\n",
    "        for t in range(time_step):\r\n",
    "            #Première prédiction faîte sur le premier espace latent\r\n",
    "            decodeur_mu_0 = self.decodeur(self.z_q_0).reshape(1,1)\r\n",
    "            # On traite l'espace latent avec le réseau de transition\r\n",
    "            trans_mu, trans_sig = self.transition(z_q_t_1)\r\n",
    "            # On ajuste notre distribution en prenant en compte les actions\r\n",
    "            post_mu, post_sig = self.posterieur(trans_mu, trans_sig, action[:, t])\r\n",
    "\r\n",
    "            z_q_t = self.reparametrization(post_mu, post_sig)\r\n",
    "            decodeur_mu = self.decodeur(z_q_t)\r\n",
    "            decodeur_mu = torch.cat((decodeur_mu_0,decodeur_mu))\r\n",
    "            #print(decodeur_mu.shape,decodeur_mu[0],obs[:, t].shape)\r\n",
    "\r\n",
    "            kl += self.kl_div(post_mu, post_sig, trans_mu, trans_sig)\r\n",
    "            reconstruction += ((decodeur_mu - obs[:, t]).pow(2).sum(dim=0) / 2 / decodeur_sig\r\n",
    "                               + self.decodeur_sigma * batch_size / 2).sum()\r\n",
    "\r\n",
    "            z_q_t_1 = z_q_t\r\n",
    "\r\n",
    "        return reconstruction, kl"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entraînement\r\n",
    "\r\n",
    "Définition des paramètres d'entraînement et entraînement sur les données."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dim_cachee = 50\r\n",
    "\r\n",
    "config_model = AttrDict({\r\n",
    "                   \"z_dim\":2,\r\n",
    "                   \"decodeur_dim_cachee\":dim_cachee,\r\n",
    "                   \"decodeur_dim_sortie\":1,\r\n",
    "                   \"decodeur_constante\":-5,\r\n",
    "                   \"transition_dim_cachee\":dim_cachee,\r\n",
    "                   \"posterieur_dim_cachee\":dim_cachee,\r\n",
    "                   \"actions_dim\":U.shape[1],\r\n",
    "                   \"device\": device,\r\n",
    "                   \"iter_num\" : 500,\r\n",
    "                   \"beta\":0.1\r\n",
    "                   })"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dkf = DeepKalmanFilter(config_model)\r\n",
    "optimizer = optim.Adam(dkf.parameters(), lr=0.001)\r\n",
    "dkf.train()\r\n",
    "dkf = dkf.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_loss, valid_losses = [],[]\r\n",
    "with tqdm.tqdm(range(config_model.iter_num)) as pbar:\r\n",
    "    for _ in pbar:\r\n",
    "        \r\n",
    "        optimizer.zero_grad()\r\n",
    "        reconstruction, kl = dkf.loss(X_train_torch,U_train_torch)\r\n",
    "        loss = reconstruction + config_model.beta * kl\r\n",
    "        train_loss.append(loss)\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        with torch.no_grad():\r\n",
    "            valid_reconstruction, valid_kl = dkf.loss(X_valid_torch,U_valid_torch)\r\n",
    "        valid_loss = valid_reconstruction + config_model.beta * valid_kl\r\n",
    "        valid_losses.append(valid_loss)\r\n",
    "        pbar.set_postfix_str(\"[train kl %d, reconstruction %d, valid_kl %d, valid_reconst %d]\"\r\n",
    "                             % (kl, reconstruction, valid_kl, valid_reconstruction))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Graphique sur le modèle\r\n",
    "\r\n",
    "On cherche à tracer qqs graphiques sur les comportements du modèle et sur ses performances de prédictions sur la série temporelle"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_graphs = AttrDict({\r\n",
    "                   \"loss\":True,\r\n",
    "                   \"test\":True,\r\n",
    "                   \"aleatoire\":False,\r\n",
    "                   \"entrainement\":False,\r\n",
    "                   \"save_figure\" : False\r\n",
    "                   })\r\n",
    "\r\n",
    "\r\n",
    "start = (N_ANNEE_DATASET-config_data.n_annee_validation-config_data.n_annee_entrainement)*N_UNITES_PAR_AN\r\n",
    "end = config_data.n_annee_entrainement*N_UNITES_PAR_AN"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "filepath = f\"./Graphs/epochs_{config_model.iter_num}_mois_{config_data.n_mois}_z_dim_{config_model.z_dim}_dimension_sous_réseaux_{dim_cachee}_beta_{config_model.beta}\"\r\n",
    "os.mkdir(filepath)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "def prediction_modele(U, model):\r\n",
    "    z = []\r\n",
    "    with torch.no_grad():\r\n",
    "        z.append(model.decodeur(model.z_q_0).item())\r\n",
    "        z_q_t_1 = model.z_q_0.expand((1, model.config.z_dim))        \r\n",
    "        for u in U :\r\n",
    "            prior_mu, prior_sig = model.transition(z_q_t_1)\r\n",
    "            z_q_t, _ = model.posterieur(prior_mu, prior_sig, u)\r\n",
    "            z_q_t_1 = z_q_t            \r\n",
    "            z.append(model.decodeur(z_q_t_1).item())       \r\n",
    "    return z\r\n",
    "\r\n",
    "def mse(Y, YH):\r\n",
    "    return '{:.3e}'.format(np.square(Y - YH).mean())\r\n",
    "\r\n",
    "def projection_sur_test (title,Z_alpha,Y_alpha):\r\n",
    "    fig = plt.figure()\r\n",
    "    plt.plot(Z_alpha, label = 'Prediction')\r\n",
    "    plt.plot(Y_alpha,label = 'Ground Truth')\r\n",
    "    plt.legend()\r\n",
    "    plt.title(title)\r\n",
    "    plt.text(len(Y_alpha)+10,0.35,f'epochs :{config_model.iter_num} ,\\nmois :{config_data.n_mois},\\nz_dim : {config_model.z_dim},\\ndim_cc : {dim_cachee},\\nBeta :{config_model.beta},\\nMSE : {mse(Z_alpha,Y_alpha)}')\r\n",
    "    plt.show()\r\n",
    "    if plot_graphs.save_figure :\r\n",
    "        fig.savefig(filepath+'/'+title+'.jpg',bbox_inches='tight')\r\n",
    "    \r\n",
    "def prediction_plus_plot(title,U,X):\r\n",
    "    Z = torch.Tensor(prediction_modele(U, dkf))\r\n",
    "    projection_sur_test(title,Z,X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if plot_graphs.loss :\r\n",
    "    \r\n",
    "    valid_losses_cpu = [i.cpu() for i in valid_losses]\r\n",
    "    train_loss_cpu = [i.cpu().detach().numpy() for i in train_loss]\r\n",
    "\r\n",
    "    fig = plt.figure()\r\n",
    "    plt.plot(train_loss_cpu,label='train loss')\r\n",
    "    plt.plot(valid_losses_cpu,label='valid loss')\r\n",
    "\r\n",
    "    #print(dkf.decodeur_sigma)\r\n",
    "\r\n",
    "    plt.legend()\r\n",
    "    plt.title('Combinaison des loss')\r\n",
    "    plt.text(config_model.iter_num+30,train_loss_cpu[-1]/2, f'epochs :{config_model.iter_num} ,\\nmois :{config_data.n_mois},\\nz_dim : {config_model.z_dim},\\ndim_cc : {dim_cachee}')\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "    if plot_graphs.save:\r\n",
    "        fig.savefig(filepath+'/Combinaison des loss.jpg',bbox_inches='tight')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    " if plot_graphs.test :\r\n",
    "\r\n",
    "   U_alpha = torch.Tensor(Big_Data[['Inflow','Rainfall','Melt of Ice']].iloc[-start:-1].to_numpy()).to(device).reshape(start-1,1,3)\r\n",
    "   X_alpha = Big_Data['Inflow'].iloc[-start:].to_numpy()\r\n",
    "\r\n",
    "   title = 'Prédictions du modèle sur le jeu de test'\r\n",
    "   prediction_plus_plot (title,U_alpha,X_alpha)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if plot_graphs.aleatoire :\r\n",
    "    \r\n",
    "    U_beta = torch.rand((start,1,3)).to(config_model.device)\r\n",
    "    X_beta = U_beta.cpu().reshape(start,3)[:,0]\r\n",
    "\r\n",
    "    title = 'Prédictions du modèle sur un jeu aléatoire'\r\n",
    "    prediction_plus_plot (title,U_beta[:-1],X_beta)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if plot_graphs.entrainement :\r\n",
    "    \r\n",
    "    U_gamma = torch.Tensor(Big_Data[['Inflow','Rainfall','Melt of Ice']].iloc[end-start:end-1].to_numpy()).to(device).reshape(start-1,1,3)\r\n",
    "    X_gamma = Big_Data['Inflow'].iloc[end-start:end].to_numpy()\r\n",
    "\r\n",
    "    title = \"Prédiction du model sur les 2 dernières années du jeu d'entraînement\"\r\n",
    "    prediction_plus_plot (title,U_gamma,X_gamma)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('TDK': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "5c96706a5f2f3768f664a6d81ff58007609e360b41383024bb8cb568cfb7ecec"
   }
  },
  "interpreter": {
   "hash": "3552b825e551c1b635478e46224b805bbcf108b43bf63b4d87d884f78905e0f4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}